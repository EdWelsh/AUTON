# AUTON Technology Stack

## Programming Languages

### Python 3.11+
**Primary orchestration language**
- Agent framework implementation
- LLM integration via LiteLLM
- Configuration management with Pydantic
- CLI interface with Click
- Git operations with GitPython

### Rust
**Build tooling and validation**
- Diff validation tools
- Kernel build system
- QEMU test runner
- Performance-critical validation components

### C/Assembly
**Generated kernel code**
- Kernel implementation (generated by agents)
- Architecture-specific assembly (NASM for x86_64, GNU AS for ARM64/RISC-V)
- Hardware abstraction layer
- Device drivers

## Core Dependencies

### Orchestration Stack
```toml
# LLM Integration
litellm = ">=1.40.0"           # Multi-provider LLM API
pydantic = ">=2.5.0"           # Configuration validation
rich = ">=13.7.0"              # Terminal UI

# Git & File Operations  
gitpython = ">=3.1.40"         # Git repository management
click = ">=8.1.0"              # CLI framework
tomli = ">=2.0.0"              # TOML parsing (Python <3.11)
```

### SLM Training Stack
```toml
# Deep Learning
torch = ">=2.0.0"              # PyTorch framework
transformers = ">=4.30.0"      # Hugging Face transformers
datasets = ">=2.12.0"          # Dataset management
tokenizers = ">=0.13.0"        # Fast tokenization
accelerate = ">=0.20.0"        # Distributed training

# Quantization & Export
bitsandbytes = ">=0.40.0"      # INT4/INT8 quantization
onnx = ">=1.14.0"              # ONNX export
onnxruntime = ">=1.15.0"       # ONNX inference
sentencepiece = ">=0.1.99"     # SentencePiece tokenization

# Analysis & Monitoring
tensorboard = ">=2.13.0"       # Training metrics
jupyter = ">=1.0.0"            # Notebook analysis
pandas = ">=2.0.0"             # Data analysis
numpy = ">=1.24.0"             # Numerical computing
matplotlib = ">=3.7.0"         # Visualization
```

### Development Tools
```toml
# Testing & Quality
pytest = ">=7.4.0"             # Unit testing
pytest-asyncio = ">=0.23.0"    # Async test support
ruff = ">=0.1.0"               # Linting and formatting
```

## Build System

### Python Package Management
- **Build System**: Hatchling
- **Package Structure**: `orchestrator` package with CLI entry point
- **Installation**: `pip install -e .` for development
- **Scripts**: `auton` CLI command

### Rust Toolchain
- **Cargo Workspace**: `agent/tools/Cargo.toml`
- **Components**: 
  - `diff-validator`: Validates agent-generated diffs
  - `kernel-builder`: Cross-compilation build system
  - `test-runner`: QEMU integration testing

### Cross-Compilation Toolchains
```bash
# x86_64
x86_64-elf-gcc, nasm, qemu-system-x86_64

# AArch64  
aarch64-elf-gcc, aarch64-elf-as, qemu-system-aarch64

# RISC-V 64
riscv64-elf-gcc, riscv64-elf-as, qemu-system-riscv64
```

## Development Commands

### Setup & Installation
```bash
# Clone and setup
git clone https://github.com/EdWelsh/AUTON.git
cd AUTON/agent
pip install -e .

# Configuration
cp config/auton.toml.example config/auton.toml
# Edit config/auton.toml with API keys
```

### Core Operations
```bash
# Run orchestration
auton run "Build bootable kernel with SLM rule engine"

# Architecture selection
auton config set kernel.arch aarch64

# Agent debugging
auton debug --agent developer --task boot-001

# Validation only
auton validate --build --test
```

### SLM Training Pipeline
```bash
# Data preparation
python SLM/scripts/prepare_data.py --dataset raw/kernel_docs

# Model training
python SLM/scripts/train.py --config configs/tiny_10M.yaml

# Quantization
python SLM/scripts/quantize.py --model checkpoints/model.pt --method gptq

# Export formats
python SLM/scripts/export_gguf.py --model quantized/model_q4.pt
python SLM/scripts/export_onnx.py --model quantized/model_q4.pt
```

### Testing & Validation
```bash
# Unit tests
pytest agent/tests/

# Integration tests  
pytest agent/kernel_spec/tests/

# QEMU validation
cargo run --bin test-runner -- --arch x86_64 --kernel kernels/x86_64/kernel.bin

# Composition validation
auton validate --composition --timeout 300
```

## Configuration Management

### Main Configuration (`auton.toml`)
```toml
[llm]
model = "anthropic/claude-opus-4-6"
max_tokens = 16384
temperature = 0.0

[agents]
developer_count = 4
reviewer_count = 1

[kernel]
arch = "x86_64"  # x86_64, aarch64, riscv64

[validation]
build_timeout = 120
test_timeout = 60
composition_checks = true
```

### Architecture Profiles
- **x86_64**: Multiboot2, NASM, ACPI, legacy PC hardware
- **AArch64**: UEFI/DTB, GNU AS, ARM-specific peripherals  
- **RISC-V**: OpenSBI, GNU AS, RISC-V standard extensions

### SLM Configuration
```toml
[slm]
model_size = "tiny_10M"  # tiny_10M, small_50M, medium_150M, large_500M
backend = "rule_engine"  # rule_engine, neural_backend
quantization = "int4"    # int4, int8, fp16

[slm.training]
batch_size = 32
learning_rate = 1e-4
max_steps = 10000
```

## LLM Provider Support

### Supported Providers (via LiteLLM)
- **Anthropic**: Claude models (`anthropic/claude-opus-4-6`)
- **OpenAI**: GPT models (`openai/gpt-4o`)
- **Ollama**: Local models (`ollama/llama3.1`)
- **Google**: Gemini models (`gemini/gemini-2.0-flash`)
- **OpenRouter**: Proxy access (`openrouter/anthropic/claude-3.5-sonnet`)
- **Azure**: Azure OpenAI (`azure/gpt-4`)

### Cost Optimization
```toml
[agents.models]
manager = "anthropic/claude-opus-4-6"      # High-capability for planning
developer = "anthropic/claude-sonnet-4-5"  # Cost-effective for coding
reviewer = "openai/gpt-4o"                 # Alternative provider
tester = "ollama/llama3.1"                 # Free local model
```

## Development Environment

### Required Tools
- **Python 3.11+**: Core runtime
- **Rust**: Build tooling
- **Git**: Version control and agent collaboration
- **QEMU**: Kernel testing
- **Cross-compilation toolchains**: Architecture-specific builds

### Optional Tools
- **Jupyter**: SLM analysis notebooks
- **TensorBoard**: Training monitoring
- **Docker**: Containerized build environments
- **VS Code/Amazon Q**: IDE integration

### Performance Considerations
- **Parallel Agents**: 4+ developer agents for throughput
- **GPU Support**: Optional for SLM training (CUDA/ROCm)
- **Memory**: 16GB+ recommended for large model training
- **Storage**: Fast SSD for frequent Git operations