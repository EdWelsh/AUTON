model:
  name: "auton-slm-tiny"
  parameters: 10_000_000
  
architecture:
  hidden_size: 256
  num_layers: 6
  num_attention_heads: 4
  num_key_value_heads: 2  # GQA
  intermediate_size: 1024
  vocab_size: 32000
  max_position_embeddings: 2048
  
  activation: "swiglu"
  rope_theta: 10000.0
  attention_dropout: 0.0
  hidden_dropout: 0.0
  
training:
  batch_size: 32
  learning_rate: 3e-4
  weight_decay: 0.1
  warmup_steps: 1000
  max_steps: 10000
  gradient_accumulation_steps: 1
  
  optimizer: "adamw"
  scheduler: "cosine"
  
  checkpoint_every: 1000
  eval_every: 500
