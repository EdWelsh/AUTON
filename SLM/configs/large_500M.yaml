model:
  name: "auton-slm-large"
  parameters: 500_000_000

architecture:
  hidden_size: 1024
  num_layers: 24
  num_attention_heads: 16
  num_key_value_heads: 4  # GQA
  intermediate_size: 4096
  vocab_size: 32000
  max_position_embeddings: 2048

  activation: "swiglu"
  rope_theta: 10000.0
  attention_dropout: 0.0
  hidden_dropout: 0.0

training:
  batch_size: 16
  learning_rate: 1e-4
  weight_decay: 0.1
  warmup_steps: 5000
  max_steps: 200000
  gradient_accumulation_steps: 8

  optimizer: "adamw"
  scheduler: "cosine"

  checkpoint_every: 10000
  eval_every: 5000
