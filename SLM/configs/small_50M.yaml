model:
  name: "auton-slm-small"
  parameters: 50_000_000
  
architecture:
  hidden_size: 512
  num_layers: 12
  num_attention_heads: 8
  num_key_value_heads: 4
  intermediate_size: 2048
  vocab_size: 32000
  max_position_embeddings: 2048
  
  activation: "swiglu"
  rope_theta: 10000.0
  attention_dropout: 0.0
  hidden_dropout: 0.0
  
training:
  batch_size: 64
  learning_rate: 2e-4
  weight_decay: 0.1
  warmup_steps: 2000
  max_steps: 50000
  gradient_accumulation_steps: 2
  
  optimizer: "adamw"
  scheduler: "cosine"
  
  checkpoint_every: 5000
  eval_every: 1000
