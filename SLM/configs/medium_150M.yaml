model:
  name: "auton-slm-medium"
  parameters: 150_000_000

architecture:
  hidden_size: 768
  num_layers: 12
  num_attention_heads: 12
  num_key_value_heads: 4  # GQA
  intermediate_size: 3072
  vocab_size: 32000
  max_position_embeddings: 2048

  activation: "swiglu"
  rope_theta: 10000.0
  attention_dropout: 0.0
  hidden_dropout: 0.0

training:
  batch_size: 32
  learning_rate: 1.5e-4
  weight_decay: 0.1
  warmup_steps: 3000
  max_steps: 100000
  gradient_accumulation_steps: 4

  optimizer: "adamw"
  scheduler: "cosine"

  checkpoint_every: 5000
  eval_every: 2000
